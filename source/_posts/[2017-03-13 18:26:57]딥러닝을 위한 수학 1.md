---
title: "딥러닝을 위한 수학 1"
date: 2017-03-13 18:26:57
tags: github, github-trend, tech-trend 
---


원문은 아래에 지속적으로 업데이트

[**ehrudxo/til**  
_til - today I learned. 오늘 공부한것을 위키 형식으로 적기시작 해봄._github.com][anchor0][][anchor1]

수학은 워낙 넓으니까 선형대수를 다 다시 공부할 수도 없고... 주로 나오는 것만 위주로 나열하기로 함.

딥러닝에 왜 이런 여러가지가 나오는지를 먼저 이해하는 것이 중요함. 다른 대부분의 강의나 튜토리얼은 그런 설명없이 바로 수식을 전개함. 너무 간단해서 언급을 안하는 거 같은데, 나 같은 더미를 위해서 이야기 하면 딥러닝을 통해 머신이 내리는 결정은 대부분이 통계와 수학식을 통해 전개됨.

통계를 위해서는 확률을 알아야 하고 이 결정을 위한 확률의 기본이 베이즈 이론이라고 보면 됨. 이후 여러가지 알고리즘과 방법들이 나오는 것도 결국 이 확률이론이 토대가 된다.

뉴럴 네트워크가 들어가는 경우는 부분에 쓰이는 수학은 변수와 모델의 통제라는 측면에서 미분 그 중에서도 편미분이 중요하고 레이어의 계산이 한꺼번에 이루어지는 관점에서는 행렬식이 너무나 중요하다. 정도로만 개념을 잡고 (일단은) 넘어가면 된다.

결국은 문제를 어떻게 정의하는가가 핵심이고 그 문제를 푸는 방법으로써의 딥러닝이 각광을 받고 있다고 보면 내가 가지고 있는 해결해야 하는 문제들을 수학적으로 어떻게 정의하는지를 알아야 하는 것이 첫번째, 그리고 그것을 해결하는 다양한 방법들을 배우는 것을 두번째라고 생각하면 된다.

일단 확률에 대해서 알아보자.

### 조건부 확률

* P(Y=y|X=x) : 구름(X)이 꼈을때(true) 비(Y)가 올 확률을 의미함. --- 출처 [위키피디아 : 조건부 확률][anchor2]
* P(E|F) = P(E and F)/P(F)
* 곱셈 정리 : P(E and F) = P(E|F) \* P(F)
* 치환 정리 : P(E|F) \* P(F) = P(F|E) \* P(E)

#### 한 개의 주사위를 두 번 던질 때 합이 3이 나올 확률 vs 두 개의 주사위를 던져서 합이 3이 나올 확률 :

* (1/6\*1/6)/1/6 = 1/6 vs 2/36 = 1/18
* 표본 공간이 바뀌었다(축소 표본 공간)

#### 1개의 주사위를 네번 굴려서 6이 한번 나오는 경우 vs 2개의 주사위를 24번 굴려서 동시에 6이 한번 나오는 경우 ( 드 메레 문제 )

* 여사건 정의(1-P(E))를 통해서 풀어야 하는데 그 이유는 처음에 6이 나오면 표본공간 자체가 필요없어지기 때문에 절대 안나오는 확률을 기준으로 계산을 한 다음에 여사건 정의로 푼다 1-(5/6)⁴ = 0.518 : 같은 방법.
* 표본공간을 확보하는 것이 관건
* 2개의 주사위를 굴려서 동시에 안 나오는 경우는 1 -(35/36)²⁴ = 0.491 --- "세상에서 가장 재미있는 통계학"

### 베이즈 정리

* 개요 : 베이즈 정리는 1740년대의 영국의 목사인 토머스 베이즈(Thomas Bayes)가 정립한, 조건부 확률에 대한 수학적 정리이다. 사건 A가 있고 사건 B가 있을 때 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률을 구하고 싶다. 그런데 지금 알고 있는 것은 사건 A가 일어난 것을 전제로 한 사건 B의 조건부 확률, A의 확률, B의 확률뿐이다. 그럴 때 다음과 같이 구할 수가 있다. 출처 : [나무위키][anchor3] .
* 조금 더 수식으로 풀어 쓰면

![][image0]출처 [나무 위키 베이즈 정리][anchor3]

* 인구 1000명당 한 명꼴로 걸리는 희귀병. 병에 걸린 사람의 경우는 90% 양성, 건강한 사람의 2% 가 양성 -\> 양성반응을 보인 사람의 병에 걸릴 확률은?
* 즉, 인간의 사고는 처음에는 아무 정보가 없던 상태에서 **새로운 정보를 받아들이고, 이를 통해 자신이 가지고 있던 일종의 사전 확률 체계를 업데이트시켜 세상을 해석하거나 판단을 내리고 의사결정을 하는 방향으로 발전되어 왔다는 것이다**. 그리고 이렇게 발전된 사후 확률 체계는 새로운 사전 확률이 되어, 새로운 정보가 유입될 때마다 업데이트를 반복해간다. --- 나무위키 인지과학 및 인공지능에서의 베이즈 정리

By [Keen Dev][anchor4] on [March 13, 2017][anchor5].

Exported from [Medium][anchor6] on May 31, 2017\.


[anchor0]: https://github.com/ehrudxo/til "https://github.com/ehrudxo/til"
[anchor1]: https://github.com/ehrudxo/til
[anchor2]: https://ko.wikipedia.org/wiki/%EC%A1%B0%EA%B1%B4%EB%B6%80_%ED%99%95%EB%A5%A0
[anchor3]: https://namu.wiki/w/%EB%B2%A0%EC%9D%B4%EC%A6%88%20%EC%A0%95%EB%A6%AC
[anchor4]: https://medium.com/@keendev
[anchor5]: https://medium.com/p/c5e1cb31a7ca
[anchor6]: https://medium.com


[image0]: /images/0*s9FyujgqEUU8HAKf.jp